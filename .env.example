# ============================================================================
# CodexAI Environment Configuration
# ============================================================================
# Copy this file to .env and configure with your values
#
# This application supports multiple LLM providers and storage options.
# Choose the configuration that best fits your deployment scenario.
# ============================================================================

# ----------------------------------------------------------------------------
# Node Environment
# ----------------------------------------------------------------------------
NODE_ENV=production
PORT=3000

# ----------------------------------------------------------------------------
# Database Configuration (Required)
# ----------------------------------------------------------------------------
# Format: mysql://username:password@host:port/database_name
DATABASE_URL=mysql://codexappuser:your_password@localhost:3306/codexapp

# ----------------------------------------------------------------------------
# JWT Secret for Authentication (Required - CHANGE IN PRODUCTION!)
# ----------------------------------------------------------------------------
JWT_SECRET=your-super-secret-jwt-key-change-me-in-production-min-32-chars

# ----------------------------------------------------------------------------
# Application ID
# ----------------------------------------------------------------------------
VITE_APP_ID=codexai

# ============================================================================
# LLM PROVIDER CONFIGURATION
# ============================================================================
# Choose ONE of the following providers: openai, claude, deepseek, ollama, forge, or custom
# Each provider has its own configuration section below
# ============================================================================

LLM_PROVIDER=openai

# ----------------------------------------------------------------------------
# OPTION 1: OpenAI Configuration (Recommended for production)
# ----------------------------------------------------------------------------
# Use official OpenAI API for GPT models
# Pros: Best quality, reliable, well-documented
# Cons: Requires API key, costs per token
# ----------------------------------------------------------------------------
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4
OPENAI_EMBEDDING_MODEL=text-embedding-3-small

# Available OpenAI models:
# - gpt-4 (most capable, higher cost)
# - gpt-4-turbo (faster, lower cost)
# - gpt-3.5-turbo (fast, economical)
#
# Available embedding models:
# - text-embedding-3-small (1536 dimensions, fast)
# - text-embedding-3-large (3072 dimensions, better quality)
# - text-embedding-ada-002 (1536 dimensions, legacy)

# ----------------------------------------------------------------------------
# OPTION 2: Claude (Anthropic) Configuration - NEW! üéâ
# ----------------------------------------------------------------------------
# Use Claude API for advanced reasoning and analysis
# Pros: Excellent reasoning, long context (200K tokens), strong at analysis
# Cons: Requires API key, costs per token
# ----------------------------------------------------------------------------
# LLM_PROVIDER=claude
# CLAUDE_API_KEY=sk-ant-your-claude-api-key-here
# CLAUDE_MODEL=claude-3-5-sonnet-20241022
# CLAUDE_BASE_URL=https://api.anthropic.com

# Available Claude models:
# - claude-3-5-sonnet-20241022 (best overall, recommended)
# - claude-3-opus-20240229 (most capable, highest cost)
# - claude-3-sonnet-20240229 (balanced performance/cost)
# - claude-3-haiku-20240307 (fastest, most economical)
#
# Get API key: https://console.anthropic.com/
# Pricing: https://www.anthropic.com/pricing
# Note: Claude uses its own SDK, embeddings still use OpenAI

# ----------------------------------------------------------------------------
# OPTION 3: DeepSeek Configuration - NEW! üéâ
# ----------------------------------------------------------------------------
# Use DeepSeek API for cost-effective Chinese LLM
# Pros: Very cost-effective (~$0.14/1M tokens), good performance
# Cons: Requires API key, newer provider
# ----------------------------------------------------------------------------
# LLM_PROVIDER=deepseek
# DEEPSEEK_API_KEY=sk-your-deepseek-api-key-here
# DEEPSEEK_MODEL=deepseek-chat
# DEEPSEEK_BASE_URL=https://api.deepseek.com/v1

# Available DeepSeek models:
# - deepseek-chat (general purpose, recommended)
# - deepseek-coder (specialized for code generation)
#
# Get API key: https://platform.deepseek.com/
# Pricing: Very competitive, significantly cheaper than OpenAI/Claude
# Note: OpenAI-compatible API, works with standard embeddings

# ----------------------------------------------------------------------------
# OPTION 4: Ollama Configuration (Best for local/self-hosted)
# ----------------------------------------------------------------------------
# Use Ollama for local open-source LLMs (Llama, Mistral, etc.)
# Pros: Free, private, no API costs, runs locally
# Cons: Requires local GPU, slower than cloud APIs
# ----------------------------------------------------------------------------
# LLM_PROVIDER=ollama
# OLLAMA_BASE_URL=http://localhost:11434/v1
# OLLAMA_MODEL=llama3
# OLLAMA_EMBEDDING_MODEL=nomic-embed-text

# How to setup Ollama:
# 1. Install Ollama: https://ollama.ai
# 2. Pull models: ollama pull llama3 && ollama pull nomic-embed-text
# 3. Ollama runs on http://localhost:11434 by default
#
# Available Ollama models (examples):
# - llama3 (Meta's Llama 3, 8B or 70B)
# - mistral (Mistral 7B, fast and capable)
# - codellama (specialized for code)
# - mixtral (Mixtral 8x7B, very capable)
# - phi (Microsoft Phi-2, small and fast)
#
# Available Ollama embedding models:
# - nomic-embed-text (768 dimensions, recommended)
# - all-minilm (384 dimensions, fast)
# - mxbai-embed-large (1024 dimensions, better quality)

# ----------------------------------------------------------------------------
# OPTION 5: Manus Forge Configuration (Optional)
# ----------------------------------------------------------------------------
# Use Manus Forge API (Gemini models)
# Pros: Integrated with Manus platform
# Cons: Requires Manus account
# ----------------------------------------------------------------------------
# LLM_PROVIDER=forge
# BUILT_IN_FORGE_API_URL=https://forge.manus.im
# BUILT_IN_FORGE_API_KEY=your-forge-api-key

# ----------------------------------------------------------------------------
# OPTION 6: Custom OpenAI-Compatible API
# ----------------------------------------------------------------------------
# Use any OpenAI-compatible API (Azure OpenAI, LocalAI, vLLM, etc.)
# Pros: Flexible, can use any compatible provider
# Cons: Requires compatible API endpoint
# ----------------------------------------------------------------------------
# LLM_PROVIDER=custom
# CUSTOM_LLM_BASE_URL=https://your-custom-api.com/v1
# CUSTOM_LLM_API_KEY=your-custom-api-key
# CUSTOM_LLM_MODEL=gpt-3.5-turbo
# CUSTOM_LLM_EMBEDDING_MODEL=text-embedding-ada-002

# Examples of custom providers:
# - Azure OpenAI: https://your-resource.openai.azure.com/openai/deployments
# - LocalAI: http://localhost:8080/v1
# - vLLM: http://localhost:8000/v1
# - LM Studio: http://localhost:1234/v1
# - Text Generation WebUI: http://localhost:5000/v1

# ============================================================================
# STORAGE PROVIDER CONFIGURATION
# ============================================================================
# Choose ONE: local (default) or s3
# ============================================================================

STORAGE_PROVIDER=local

# ----------------------------------------------------------------------------
# Local Storage Configuration (Default, recommended for getting started)
# ----------------------------------------------------------------------------
# Stores files on the local filesystem
# Pros: Simple, no external dependencies, free
# Cons: Not suitable for distributed deployments
# ----------------------------------------------------------------------------
LOCAL_STORAGE_PATH=./storage

# ----------------------------------------------------------------------------
# S3 Storage Configuration (Optional, for production)
# ----------------------------------------------------------------------------
# Use Amazon S3 or S3-compatible storage (MinIO, DigitalOcean Spaces, etc.)
# Pros: Scalable, distributed, backup-friendly
# Cons: Requires S3 account, costs per GB
# ----------------------------------------------------------------------------
# STORAGE_PROVIDER=s3
# S3_BUCKET=your-bucket-name
# S3_REGION=us-east-1
# S3_ACCESS_KEY=your-access-key
# S3_SECRET_KEY=your-secret-key

# For S3-compatible services (MinIO, DigitalOcean Spaces):
# Set S3_ENDPOINT to your service URL
# S3_ENDPOINT=https://your-minio-server.com

# ============================================================================
# ADMIN USER CONFIGURATION (For Database Seeding)
# ============================================================================
# These credentials are used to create the initial admin user
# Run: pnpm db:seed
# ============================================================================

ADMIN_EMAIL=admin@codexai.local
ADMIN_PASSWORD=Admin123!
ADMIN_NAME=Administrator

# ‚ö†Ô∏è IMPORTANT: Change these credentials after first login!

# ============================================================================
# OPTIONAL: MANUS OAUTH (Backward Compatibility)
# ============================================================================
# If you want to use Manus OAuth in addition to local auth
# Leave empty to use only local authentication
# ============================================================================

OAUTH_SERVER_URL=
OWNER_OPEN_ID=

# ============================================================================
# DEPLOYMENT EXAMPLES
# ============================================================================

# ----------------------------------------------------------------------------
# Example 1: Local Development with Ollama (Free, Private)
# ----------------------------------------------------------------------------
# NODE_ENV=development
# LLM_PROVIDER=ollama
# OLLAMA_BASE_URL=http://localhost:11434/v1
# OLLAMA_MODEL=llama3
# OLLAMA_EMBEDDING_MODEL=nomic-embed-text
# STORAGE_PROVIDER=local
# LOCAL_STORAGE_PATH=./storage

# ----------------------------------------------------------------------------
# Example 2: Production with OpenAI (Best Quality)
# ----------------------------------------------------------------------------
# NODE_ENV=production
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-your-key
# OPENAI_MODEL=gpt-4
# STORAGE_PROVIDER=s3
# S3_BUCKET=codexai-prod
# S3_REGION=us-east-1

# ----------------------------------------------------------------------------
# Example 2b: Production with Claude (Excellent Reasoning)
# ----------------------------------------------------------------------------
# NODE_ENV=production
# LLM_PROVIDER=claude
# CLAUDE_API_KEY=sk-ant-your-key
# CLAUDE_MODEL=claude-3-5-sonnet-20241022
# STORAGE_PROVIDER=s3
# S3_BUCKET=codexai-prod
# S3_REGION=us-east-1

# ----------------------------------------------------------------------------
# Example 2c: Production with DeepSeek (Cost-Effective)
# ----------------------------------------------------------------------------
# NODE_ENV=production
# LLM_PROVIDER=deepseek
# DEEPSEEK_API_KEY=sk-your-key
# DEEPSEEK_MODEL=deepseek-chat
# STORAGE_PROVIDER=s3
# S3_BUCKET=codexai-prod
# S3_REGION=us-east-1

# ----------------------------------------------------------------------------
# Example 3: Hybrid (OpenAI for LLM, Local for Storage)
# ----------------------------------------------------------------------------
# NODE_ENV=production
# LLM_PROVIDER=openai
# OPENAI_API_KEY=sk-your-key
# OPENAI_MODEL=gpt-3.5-turbo
# STORAGE_PROVIDER=local
# LOCAL_STORAGE_PATH=/var/codexai/storage

# ----------------------------------------------------------------------------
# Example 4: Fully Self-Hosted (No External Dependencies)
# ----------------------------------------------------------------------------
# NODE_ENV=production
# LLM_PROVIDER=ollama
# OLLAMA_BASE_URL=http://localhost:11434/v1
# OLLAMA_MODEL=mistral
# STORAGE_PROVIDER=local
# LOCAL_STORAGE_PATH=/opt/codexai/storage

# ============================================================================
# SECURITY NOTES
# ============================================================================
# 1. Always use strong, unique JWT_SECRET (min 32 characters)
# 2. Never commit .env file to version control
# 3. Use environment-specific .env files (.env.production, .env.staging)
# 4. Rotate API keys and secrets regularly
# 5. Use HTTPS/TLS in production (configure reverse proxy)
# 6. Limit database user permissions to only required operations
# 7. Enable firewall rules to restrict access to database and services
# 8. Keep all dependencies updated (pnpm update)
# 9. Monitor logs for suspicious activity
# 10. Implement regular backups for database and storage

# ============================================================================
# PERFORMANCE TUNING
# ============================================================================
# For high-traffic deployments:
# - Use Redis for session caching
# - Use PostgreSQL with pgvector for better vector search
# - Use CDN for static assets
# - Enable HTTP/2 and compression in reverse proxy
# - Scale horizontally with load balancer
# - Use connection pooling for database
# - Implement rate limiting

# ============================================================================
# SUPPORT
# ============================================================================
# Documentation: See README.md and DEPLOYMENT.md
# Issues: https://github.com/KI-Ind/codexai-app/issues
# ============================================================================
